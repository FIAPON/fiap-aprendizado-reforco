{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a12971d9",
   "metadata": {},
   "source": [
    "# Tic-Tac-Toe Reinforcement Learning\n",
    "\n",
    "Este notebook tem como objetivo demosntrar o simples treinamento atrav√©s da t√©cnica de aprendizado por refor√ßo no jogo tic-tae-toe:\n",
    "- Objetivo: entender intui√ß√£o por tr√°s de *value functions* e pol√≠ticas *epsilon-greedy* aplicadas a decis√µes sequenciais.\n",
    "- Resultado: rodar e treinar um agente simples que aprende jogando tic-tac-toe e visualizar desempenho."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87abd085",
   "metadata": {},
   "source": [
    "## Contexto e Intui√ß√£o\n",
    "\n",
    "- *Value Function* (V(s)): uma estimativa do valor futuro esperado de estar em um determinado estado (ex.: posi√ß√£o do tabuleiro).\n",
    "- Atualiza√ß√£o incremental (TD-like): V(s) <- V(s) + alpha * (V(s') - V(s)). Intuitivamente, ajustamos expectativas com base no resultado observado.\n",
    "- Pol√≠tica *epsilon-greedy*: mistura entre explorar (testar novas a√ß√µes) e explorar (usar o que sabemos ser bom).\n",
    "\n",
    "Aplica√ß√£o em neg√≥cios: decis√µes sequenciais com incerteza (invent√°rio, pricing, aloca√ß√£o). O mesmo trade-off explora√ß√£o/explora√ß√£o aparece em estrat√©gias de mercado e investimento em inova√ß√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e8584d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√©lula de import e configura√ß√£o\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "LENGTH = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a7aabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defini√ß√£o das classes (Agent, Environment, Human) e fun√ß√µes auxiliares\n",
    "class Agent:\n",
    "    def __init__(self, eps=0.1, alpha=0.5):\n",
    "        self.eps = eps\n",
    "        self.alpha = alpha\n",
    "        self.verbose = False\n",
    "        self.state_history = []\n",
    "\n",
    "    def setV(self, V):\n",
    "        self.V = V\n",
    "\n",
    "    def set_symbol(self, sym):\n",
    "        self.sym = sym\n",
    "\n",
    "    def set_verbose(self, v):\n",
    "        self.verbose = v\n",
    "\n",
    "    def reset_history(self):\n",
    "        self.state_history = []\n",
    "\n",
    "    def take_action(self, env):\n",
    "        r = np.random.rand()\n",
    "        best_state = None\n",
    "\n",
    "        if r < self.eps:\n",
    "            possible_moves = []\n",
    "            for i in range(LENGTH):\n",
    "                for j in range(LENGTH):\n",
    "                    if env.is_empty(i, j):\n",
    "                        possible_moves.append((i, j))\n",
    "            idx = np.random.choice(len(possible_moves))\n",
    "            next_move = possible_moves[idx]\n",
    "        else:\n",
    "            pos2value = {}\n",
    "            next_move = None\n",
    "            best_value = -1\n",
    "\n",
    "            for i in range(LENGTH):\n",
    "                for j in range(LENGTH):\n",
    "                    if env.is_empty(i, j):\n",
    "                        env.board[i, j] = self.sym\n",
    "                        state = env.get_state()\n",
    "                        env.board[i, j] = 0\n",
    "                        pos2value[(i, j)] = self.V[state]\n",
    "\n",
    "                        if self.V[state] > best_value:\n",
    "                            best_value = self.V[state]\n",
    "                            best_state = state\n",
    "                            next_move = (i, j)\n",
    "\n",
    "        env.board[next_move[0], next_move[1]] = self.sym\n",
    "\n",
    "    def update_state_history(self, s):\n",
    "        self.state_history.append(s)\n",
    "\n",
    "    def update(self, env):\n",
    "        reward = env.reward(self.sym)\n",
    "        target = reward\n",
    "\n",
    "        for prev in reversed(self.state_history):\n",
    "            value = self.V[prev] + self.alpha * (target - self.V[prev])\n",
    "            self.V[prev] = value\n",
    "            target = value\n",
    "\n",
    "        self.reset_history()\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.board = np.zeros((LENGTH, LENGTH))\n",
    "        self.x = -1\n",
    "        self.o = 1\n",
    "        self.winner = None\n",
    "        self.ended = False\n",
    "        self.num_states = 3 ** (LENGTH * LENGTH)\n",
    "\n",
    "    def is_empty(self, i, j):\n",
    "        return self.board[i, j] == 0\n",
    "\n",
    "    def reward(self, sym):\n",
    "        if not self.game_over():\n",
    "            return 0\n",
    "        return 1 if self.winner == sym else 0\n",
    "\n",
    "    def get_state(self):\n",
    "        k = 0\n",
    "        h = 0\n",
    "        for i in range(LENGTH):\n",
    "            for j in range(LENGTH):\n",
    "                if self.board[i, j] == 0:\n",
    "                    v = 0\n",
    "                elif self.board[i, j] == self.x:\n",
    "                    v = 1\n",
    "                elif self.board[i, j] == self.o:\n",
    "                    v = 2\n",
    "                h += (3 ** k) * v\n",
    "                k += 1\n",
    "        return h\n",
    "\n",
    "    def game_over(self, force_recalculate=False):\n",
    "        if not force_recalculate and self.ended:\n",
    "            return self.ended\n",
    "\n",
    "        for i in range(LENGTH):\n",
    "            for player in (self.x, self.o):\n",
    "                if self.board[i].sum() == player * LENGTH:\n",
    "                    self.winner = player\n",
    "                    self.ended = True\n",
    "                    return True\n",
    "\n",
    "        for j in range(LENGTH):\n",
    "            for player in (self.x, self.o):\n",
    "                if self.board[:, j].sum() == player * LENGTH:\n",
    "                    self.winner = player\n",
    "                    self.ended = True\n",
    "                    return True\n",
    "\n",
    "        for player in (self.x, self.o):\n",
    "            if self.board.trace() == player * LENGTH:\n",
    "                self.winner = player\n",
    "                self.ended = True\n",
    "                return True\n",
    "\n",
    "            if np.fliplr(self.board).trace() == player * LENGTH:\n",
    "                self.winner = player\n",
    "                self.ended = True\n",
    "                return True\n",
    "\n",
    "        if np.all((self.board == 0) == False):\n",
    "            self.winner = None\n",
    "            self.ended = True\n",
    "            return True\n",
    "\n",
    "        self.winner = None\n",
    "        return False\n",
    "\n",
    "    def is_draw(self):\n",
    "        return self.ended and self.winner is None\n",
    "\n",
    "    def draw_board(self):\n",
    "        for i in range(LENGTH):\n",
    "            print(\"-------------\")\n",
    "            for j in range(LENGTH):\n",
    "                print(\"  \", end=\"\")\n",
    "                if self.board[i, j] == self.x:\n",
    "                    print(\"x \", end=\"\")\n",
    "                elif self.board[i, j] == self.o:\n",
    "                    print(\"o \", end=\"\")\n",
    "                else:\n",
    "                    print(\"  \", end=\"\")\n",
    "            print(\"\")\n",
    "        print(\"-------------\")\n",
    "\n",
    "class Human:\n",
    "    def __init__(self):\n",
    "        self.sym = None\n",
    "\n",
    "    def set_symbol(self, sym):\n",
    "        self.sym = sym\n",
    "\n",
    "    def take_action(self, env):\n",
    "        while True:\n",
    "            move = input(\"Insira as coordenadas i, j para o pr√≥ximo movimento (por exemplo: 0,2): \")\n",
    "            i, j = move.split(',')\n",
    "            i = int(i)\n",
    "            j = int(j)\n",
    "            if env.is_empty(i, j):\n",
    "                env.board[i, j] = self.sym\n",
    "                break\n",
    "\n",
    "def get_state_hash_and_winner(env, i=0, j=0):\n",
    "    results = []\n",
    "    for v in (0, env.x, env.o):\n",
    "        env.board[i, j] = v\n",
    "        if j == 2:\n",
    "            if i == 2:\n",
    "                state = env.get_state()\n",
    "                ended = env.game_over(force_recalculate=True)\n",
    "                winner = env.winner\n",
    "                results.append((state, winner, ended))\n",
    "            else:\n",
    "                results += get_state_hash_and_winner(env, i + 1, 0)\n",
    "        else:\n",
    "            results += get_state_hash_and_winner(env, i, j + 1)\n",
    "    return results\n",
    "\n",
    "def initialV_x(env, state_winner_triples):\n",
    "    V = np.zeros(env.num_states)\n",
    "    for state, winner, ended in state_winner_triples:\n",
    "        if ended:\n",
    "            if winner == env.x:\n",
    "                v = 1\n",
    "            else:\n",
    "                v = 0\n",
    "        else:\n",
    "            v = 0.5\n",
    "        V[state] = v\n",
    "    return V\n",
    "\n",
    "def initialV_o(env, state_winner_triples):\n",
    "    V = np.zeros(env.num_states)\n",
    "    for state, winner, ended in state_winner_triples:\n",
    "        if ended:\n",
    "            if winner == env.o:\n",
    "                v = 1\n",
    "            else:\n",
    "                v = 0\n",
    "        else:\n",
    "            v = 0.5\n",
    "        V[state] = v\n",
    "    return V\n",
    "\n",
    "def play_game(p1, p2, env, draw=False):\n",
    "    current_player = None\n",
    "    while not env.game_over():\n",
    "        if current_player == p1:\n",
    "            current_player = p2\n",
    "        else:\n",
    "            current_player = p1\n",
    "\n",
    "        if draw:\n",
    "            if draw == 1 and current_player == p1:\n",
    "                env.draw_board()\n",
    "            if draw == 2 and current_player == p2:\n",
    "                env.draw_board()\n",
    "\n",
    "        current_player.take_action(env)\n",
    "        state = env.get_state()\n",
    "        p1.update_state_history(state)\n",
    "        p2.update_state_history(state)\n",
    "\n",
    "    if draw:\n",
    "        env.draw_board()\n",
    "\n",
    "    p1.update(env)\n",
    "    p2.update(env)\n",
    "\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a21118c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√©lula de treino com m√©trica simples e visualiza√ß√£o\n",
    "env = Environment()\n",
    "state_winner_triples = get_state_hash_and_winner(env)\n",
    "\n",
    "Vx = initialV_x(env, state_winner_triples)\n",
    "Vo = initialV_o(env, state_winner_triples)\n",
    "\n",
    "p1 = Agent(eps=0.1, alpha=0.3)\n",
    "p2 = Agent(eps=0.1, alpha=0.3)\n",
    "p1.setV(Vx)\n",
    "p2.setV(Vo)\n",
    "p1.set_symbol(env.x)\n",
    "p2.set_symbol(env.o)\n",
    "\n",
    "T = 2000\n",
    "p1_wins = 0\n",
    "p2_wins = 0\n",
    "draws = 0\n",
    "history = []\n",
    "window = deque(maxlen=100)\n",
    "\n",
    "for t in range(T):\n",
    "    if t % 200 == 0:\n",
    "        print('Iter', t)\n",
    "    env = Environment()\n",
    "    env = play_game(p1, p2, env)\n",
    "    if env.winner == env.x:\n",
    "        p1_wins += 1\n",
    "        window.append(1)\n",
    "    elif env.winner == env.o:\n",
    "        p2_wins += 1\n",
    "        window.append(0)\n",
    "    else:\n",
    "        draws += 1\n",
    "        window.append(0)\n",
    "    history.append(np.mean(window) if len(window) else 0)\n",
    "\n",
    "print('Treino completo')\n",
    "print('p1 wins:', p1_wins, 'p2 wins:', p2_wins, 'draws:', draws)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(history)\n",
    "plt.title('M√©dia m√≥vel (janela=100) de vit√≥rias do p1 ao longo do treino')\n",
    "plt.xlabel('Partidas')\n",
    "plt.ylabel('Taxa de vit√≥ria (p1)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faef79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstra√ß√£o: agente treinado vs agente aleat√≥rio (sem aprendizado)\n",
    "class RandomAgent(Agent):\n",
    "    def take_action(self, env):\n",
    "        possible_moves = []\n",
    "        for i in range(LENGTH):\n",
    "            for j in range(LENGTH):\n",
    "                if env.is_empty(i, j):\n",
    "                    possible_moves.append((i, j))\n",
    "        idx = np.random.choice(len(possible_moves))\n",
    "        env.board[possible_moves[idx][0], possible_moves[idx][1]] = self.sym\n",
    "\n",
    "# Comparar 100 partidas\n",
    "rand = RandomAgent()\n",
    "rand.set_symbol(env.o)\n",
    "rand.setV(Vo)\n",
    "rand.set_verbose(False)\n",
    "\n",
    "p1.set_verbose(False)\n",
    "results = { 'p1':0, 'rand':0, 'draw':0 }\n",
    "N = 100\n",
    "for i in range(N):\n",
    "    game_env = Environment()\n",
    "    game_env = play_game(p1, rand, game_env)\n",
    "    if game_env.winner == game_env.x:\n",
    "        results['p1'] += 1\n",
    "    elif game_env.winner == game_env.o:\n",
    "        results['rand'] += 1\n",
    "    else:\n",
    "        results['draw'] += 1\n",
    "\n",
    "print('Resultados vs Random (N=', N, '):', results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44941d3f",
   "metadata": {},
   "source": [
    "## Discuss√£o (para MBA)\n",
    "\n",
    "- O agente aprende via atualiza√ß√µes incrementais em estados visitados durante um epis√≥dio. Isso √© similar a atualizar estimativas de valor de projetos com novas evid√™ncias.\n",
    "- Ajustar `alpha` e `eps` controla velocidade de aprendizagem e propens√£o a experimentar ‚Äî an√°logo a or√ßamento para testes e explora√ß√£o de novos produtos.\n",
    "- Pr√≥ximos passos pr√°ticos: modelar estados e recompensas reais do neg√≥cio, simular pol√≠ticas e estimar custo de experimenta√ß√£o."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e16c6c",
   "metadata": {},
   "source": [
    "## Jogar contra o Agente Treinado\n",
    "\n",
    "Agora voc√™ pode jogar contra o agente treinado! Voc√™ ser√° o jogador 'O' e o agente treinado ser√° o jogador 'X'.\n",
    "\n",
    "**Como jogar:**\n",
    "- Voc√™ faz o primeiro movimento ap√≥s o agente\n",
    "- Digite as coordenadas no formato `linha,coluna` (ex: `0,0`, `1,2`)\n",
    "- Posi√ß√µes v√°lidas s√£o de (0,0) a (2,2)\n",
    "- O tabuleiro √© exibido com 'x' para o agente, 'o' para voc√™, e '  ' para c√©lulas vazias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2f224b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game_with_human(agent, human, env, verbose=False):\n",
    "    \"\"\"\n",
    "    Fun√ß√£o para jogar uma partida com um jogador humano.\n",
    "    O agente escolhe primeiro (X), e o humano joga com O.\n",
    "    \"\"\"\n",
    "    current_player = None\n",
    "    turn_count = 0\n",
    "\n",
    "    while not env.game_over():\n",
    "        # Alternar entre jogadores\n",
    "        if current_player == agent:\n",
    "            current_player = human\n",
    "        else:\n",
    "            current_player = agent\n",
    "\n",
    "        # Mostrar tabuleiro antes do movimento\n",
    "        if current_player == human or (current_player == agent and turn_count == 0):\n",
    "            print(\"\\n[Tabuleiro]\")\n",
    "            env.draw_board()\n",
    "\n",
    "        # Jogador atual faz um movimento\n",
    "        if isinstance(current_player, Human):\n",
    "            print(f\"\\n[Sua vez (O)]\")\n",
    "            current_player.take_action(env)\n",
    "        else:\n",
    "            print(f\"\\n[Vez do Agente (X)]\")\n",
    "            agent.take_action(env)\n",
    "\n",
    "        # Atualizar hist√≥rico de estados\n",
    "        state = env.get_state()\n",
    "        agent.update_state_history(state)\n",
    "        human.update_state_history(state) if hasattr(human, 'update_state_history') else None\n",
    "        turn_count += 1\n",
    "\n",
    "    # Mostrar tabuleiro final\n",
    "    print(\"\\n[Tabuleiro Final]\")\n",
    "    env.draw_board()\n",
    "\n",
    "    # Determinar resultado\n",
    "    if env.winner == agent.sym:\n",
    "        print(\"\\nü§ñ AGENTE VENCEU!\")\n",
    "    elif env.winner == human.sym:\n",
    "        print(\"\\nüéâ VOC√ä VENCEU!\")\n",
    "    else:\n",
    "        print(\"\\nü§ù EMPATE!\")\n",
    "\n",
    "    return env\n",
    "\n",
    "# Preparar para jogar\n",
    "print(\"Agente treinado pronto para jogar!\")\n",
    "print(\"Voc√™ joga com 'O', o agente joga com 'X'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe4aafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jogar partidas com o usu√°rio\n",
    "human = Human()\n",
    "human.set_symbol(env.o)\n",
    "p1.set_verbose(False)\n",
    "\n",
    "# Loop para jogar m√∫ltiplas partidas\n",
    "play_again = True\n",
    "game_count = 0\n",
    "human_wins = 0\n",
    "agent_wins = 0\n",
    "game_draws = 0\n",
    "\n",
    "while play_again:\n",
    "    game_count += 1\n",
    "    print(f\"\\n=== PARTIDA {game_count} ===\")\n",
    "    \n",
    "    game_env = Environment()\n",
    "    final_env = play_game_with_human(p1, human, game_env)\n",
    "    \n",
    "    # Registrar resultado\n",
    "    if final_env.winner == final_env.x:\n",
    "        agent_wins += 1\n",
    "    elif final_env.winner == final_env.o:\n",
    "        human_wins += 1\n",
    "    else:\n",
    "        game_draws += 1\n",
    "    \n",
    "    # Perguntar se deseja jogar novamente\n",
    "    response = input(\"\\nDeseja jogar novamente? (s/n): \").strip().lower()\n",
    "    if response != 's' and response != 'sim':\n",
    "        play_again = False\n",
    "\n",
    "# Exibir placar final\n",
    "print(f\"\\n=== PLACAR FINAL ===\")\n",
    "print(f\"Voc√™ (O): {human_wins} vit√≥rias\")\n",
    "print(f\"Agente (X): {agent_wins} vit√≥rias\")\n",
    "print(f\"Empates: {game_draws}\")\n",
    "print(f\"Total de partidas: {game_count}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
