{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "715b8a8a",
   "metadata": {},
   "source": [
    "# üéØ Value Iteration em Labirinto - MDP (Markov Decision Process)\n",
    "\n",
    "## Objetivo Educacional\n",
    "\n",
    "Demonstrar como um **agente** aprende a encontrar o caminho √≥timo em um labirinto usando:\n",
    "- üìä **Value Iteration** (Itera√ß√£o de Valor)\n",
    "- üîÑ **Equa√ß√£o de Bellman**\n",
    "- üé≤ **Processos de Decis√£o Markovianos (MDP)**\n",
    "\n",
    "## O Problema\n",
    "\n",
    "Um agente est√° preso em um **labirinto** e precisa encontrar a sa√≠da (objetivo).\n",
    "\n",
    "- üü´ **Paredes (1)**: N√£o √© poss√≠vel atravessar\n",
    "- ‚ö™ **Caminhos Livres (0)**: Espa√ßos onde o agente pode andar\n",
    "- üéØ **Objetivo (9)**: A meta a alcan√ßar\n",
    "\n",
    "## A Solu√ß√£o: Value Iteration\n",
    "\n",
    "O algoritmo calcula o **valor de cada estado** usando:\n",
    "\n",
    "$$V(s) = \\max_a \\sum_{s'} P(s'|s,a) [R(s,a,s') + \\gamma \\cdot V(s')]$$\n",
    "\n",
    "**Simplificado para nosso caso:**\n",
    "$$V(s) = \\max_a [\\gamma \\cdot (0.8 \\cdot V(s_{pr√≥ximo}) + 0.2 \\cdot V(s))]$$\n",
    "\n",
    "Onde:\n",
    "- **V(s)**: Valor do estado atual (quanto de recompensa se espera da√≠)\n",
    "- **0.8**: Probabilidade de sucesso na transi√ß√£o (80%)\n",
    "- **0.2**: Probabilidade de falha (ficar no mesmo lugar = 20%)\n",
    "- **Œ≥ = 0.9**: Fator de desconto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f92b403",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Importa√ß√µes e Configura√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03802114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Par√¢metros do MDP\n",
    "gamma = 0.9  # Fator de desconto\n",
    "transition_success = 0.8  # Probabilidade de sucesso na a√ß√£o\n",
    "transition_failure = 0.2  # Probabilidade de falha (ficar no mesmo lugar)\n",
    "\n",
    "# ============ DEFINI√á√ÉO DO LABIRINTO ============\n",
    "# 0 = Caminho livre\n",
    "# 1 = Parede\n",
    "# 9 = Objetivo\n",
    "maze = np.array([\n",
    "    [1, 1, 1, 1, 1, 1],\n",
    "    [1, 0, 1, 0, 0, 1],\n",
    "    [1, 0, 1, 0, 1, 1],\n",
    "    [1, 0, 0, 0, 0, 1],\n",
    "    [1, 0, 1, 0, 0, 1],\n",
    "    [1, 0, 0, 1, 9, 1],\n",
    "    [1, 1, 1, 1, 1, 1]\n",
    "])\n",
    "\n",
    "num_rows, num_cols = maze.shape\n",
    "\n",
    "# Inicializar matriz de valores\n",
    "V = np.zeros((num_rows, num_cols))\n",
    "\n",
    "# Coordenadas do objetivo\n",
    "goal_pos = np.where(maze == 9)\n",
    "goal_row, goal_col = goal_pos[0][0], goal_pos[1][0]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LABIRINTO - VALUE ITERATION (MDP)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüìê Dimens√µes: {num_rows} x {num_cols}\")\n",
    "print(f\"üéØ Objetivo em: ({goal_row}, {goal_col})\")\n",
    "print(f\"‚öñÔ∏è  Fator de Desconto (Œ≥): {gamma}\")\n",
    "print(f\"‚úÖ Prob. de Sucesso: {transition_success*100}% | ‚ùå Prob. Falha: {transition_failure*100}%\")\n",
    "print(\"\\nMapa do Labirinto:\")\n",
    "print(\"  1 = Parede | 0 = Caminho | 9 = Objetivo\")\n",
    "print(maze)\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4b8763",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Fun√ß√µes de C√°lculo - Equa√ß√£o de Bellman\n",
    "\n",
    "### L√≥gica do MDP\n",
    "\n",
    "Para cada estado e a√ß√£o:\n",
    "1. Determinar o pr√≥ximo estado se a a√ß√£o tiver sucesso (80% chance)\n",
    "2. Determinar o estado se a a√ß√£o falhar (20% chance = fica no mesmo lugar)\n",
    "3. Calcular valor = Œ≥ √ó (0.8 √ó V(pr√≥ximo) + 0.2 √ó V(atual))\n",
    "4. Pegar o m√°ximo entre todas as 4 a√ß√µes poss√≠veis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c019817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_state(state, action):\n",
    "    \"\"\"\n",
    "    Retorna o pr√≥ximo estado dado uma a√ß√£o.\n",
    "    \n",
    "    Par√¢metros:\n",
    "    - state: tupla (row, col)\n",
    "    - action: str ('up', 'down', 'left', 'right')\n",
    "    \n",
    "    Retorno:\n",
    "    - tupla (new_row, new_col)\n",
    "    \"\"\"\n",
    "    row, col = state\n",
    "    \n",
    "    if action == 'up':\n",
    "        return (max(row - 1, 0), col)\n",
    "    elif action == 'down':\n",
    "        return (min(row + 1, num_rows - 1), col)\n",
    "    elif action == 'left':\n",
    "        return (row, max(col - 1, 0))\n",
    "    elif action == 'right':\n",
    "        return (row, min(col + 1, num_cols - 1))\n",
    "    \n",
    "    return state\n",
    "\n",
    "def calculate_action_value(state, action):\n",
    "    \"\"\"\n",
    "    Calcula o valor de uma a√ß√£o em um determinado estado.\n",
    "    \n",
    "    Usa a f√≥rmula do MDP:\n",
    "    V(s,a) = Œ≥ √ó [0.8 √ó V(s') + 0.2 √ó V(s)]\n",
    "    \n",
    "    Par√¢metros:\n",
    "    - state: tupla (row, col)\n",
    "    - action: str ('up', 'down', 'left', 'right')\n",
    "    \n",
    "    Retorno:\n",
    "    - float: valor calculado\n",
    "    \"\"\"\n",
    "    row, col = state\n",
    "    \n",
    "    # N√£o calcular para paredes ou objetivo j√° alcan√ßado\n",
    "    if maze[row, col] == 1:\n",
    "        return 0\n",
    "    if maze[row, col] == 9:  # Se j√° est√° no objetivo\n",
    "        return 1\n",
    "    \n",
    "    # Pr√≥ximo estado se a a√ß√£o tiver sucesso (80%)\n",
    "    next_state = get_next_state(state, action)\n",
    "    next_row, next_col = next_state\n",
    "    \n",
    "    # Par√¢metros da equa√ß√£o de Bellman\n",
    "    # Recompensa imediata + valor descontado do pr√≥ximo estado\n",
    "    action_value = gamma * (\n",
    "        transition_success * V[next_row, next_col] +  # 80% de sucesso\n",
    "        transition_failure * V[row, col]               # 20% de falha (fica no lugar)\n",
    "    )\n",
    "    \n",
    "    return action_value\n",
    "\n",
    "def value_iteration(max_iterations=50, tolerance=1e-2):\n",
    "    \"\"\"\n",
    "    Executa o algoritmo de itera√ß√£o de valor.\n",
    "    \n",
    "    Par√¢metros:\n",
    "    - max_iterations: n√∫mero m√°ximo de itera√ß√µes\n",
    "    - tolerance: crit√©rio de parada (converg√™ncia)\n",
    "    \n",
    "    Retorno:\n",
    "    - history: hist√≥rico de itera√ß√µes\n",
    "    \"\"\"\n",
    "    global V\n",
    "    \n",
    "    history = []\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        delta = 0\n",
    "        V_old = V.copy()\n",
    "        \n",
    "        # Para cada estado no labirinto\n",
    "        for row in range(num_rows):\n",
    "            for col in range(num_cols):\n",
    "                # Ignorar paredes\n",
    "                if maze[row, col] == 1:\n",
    "                    continue\n",
    "                \n",
    "                # Objetivo tem valor 1\n",
    "                if maze[row, col] == 9:\n",
    "                    V[row, col] = 1.0\n",
    "                    continue\n",
    "                \n",
    "                # Calcular valor m√°ximo entre todas as a√ß√µes\n",
    "                max_value = -np.inf\n",
    "                \n",
    "                for action in ['up', 'down', 'left', 'right']:\n",
    "                    action_value = calculate_action_value((row, col), action)\n",
    "                    max_value = max(max_value, action_value)\n",
    "                \n",
    "                V[row, col] = max_value\n",
    "                delta = max(delta, abs(V_old[row, col] - V[row, col]))\n",
    "        \n",
    "        history.append({\n",
    "            'iteration': iteration + 1,\n",
    "            'delta': delta,\n",
    "            'V': V.copy()\n",
    "        })\n",
    "        \n",
    "        print(f\"Itera√ß√£o {iteration + 1:2d}: Delta = {delta:.6f}\")\n",
    "        \n",
    "        # Crit√©rio de converg√™ncia\n",
    "        if delta < tolerance:\n",
    "            print(f\"\\n‚úÖ Converg√™ncia alcan√ßada em {iteration + 1} itera√ß√µes!\")\n",
    "            break\n",
    "    \n",
    "    return history\n",
    "\n",
    "print(\"‚úì Fun√ß√µes de c√°lculo definidas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a826df",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Fun√ß√µes de Visualiza√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec18d570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_value_function(V, title=\"Fun√ß√£o de Valor - Labirinto\", iteration=None):\n",
    "    \"\"\"\n",
    "    Visualiza a fun√ß√£o de valor usando matplotlib.\n",
    "    \n",
    "    Par√¢metros:\n",
    "    - V: matriz de valores\n",
    "    - title: t√≠tulo do gr√°fico\n",
    "    - iteration: n√∫mero da itera√ß√£o (opcional)\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Criar colormap customizado\n",
    "    cmap = plt.cm.RdYlGn\n",
    "    \n",
    "    # Visualizar valores\n",
    "    im = ax.imshow(np.where(maze == 1, np.nan, V), cmap=cmap, vmin=0, vmax=1)\n",
    "    \n",
    "    # Desenhar paredes\n",
    "    for row in range(num_rows):\n",
    "        for col in range(num_cols):\n",
    "            if maze[row, col] == 1:\n",
    "                rect = patches.Rectangle((col-0.5, row-0.5), 1, 1, \n",
    "                                        linewidth=0, facecolor='black', alpha=0.5)\n",
    "                ax.add_patch(rect)\n",
    "    \n",
    "    # Adicionar valores nas c√©lulas\n",
    "    for row in range(num_rows):\n",
    "        for col in range(num_cols):\n",
    "            if maze[row, col] != 1:\n",
    "                if maze[row, col] == 9:\n",
    "                    ax.text(col, row, '‚òÖ\\nOBJ', ha='center', va='center', \n",
    "                           fontsize=10, fontweight='bold', color='white')\n",
    "                else:\n",
    "                    ax.text(col, row, f'{V[row, col]:.2f}', ha='center', va='center',\n",
    "                           fontsize=9, fontweight='bold', \n",
    "                           color='black' if V[row, col] < 0.5 else 'white')\n",
    "    \n",
    "    # Configurar eixos\n",
    "    ax.set_xticks(range(num_cols))\n",
    "    ax.set_yticks(range(num_rows))\n",
    "    ax.set_xlim(-0.5, num_cols-0.5)\n",
    "    ax.set_ylim(num_rows-0.5, -0.5)\n",
    "    \n",
    "    # T√≠tulo\n",
    "    if iteration:\n",
    "        title = f\"{title}\\nItera√ß√£o: {iteration}\"\n",
    "    ax.set_title(title, fontsize=13, fontweight='bold')\n",
    "    \n",
    "    # Colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax, label='Valor do Estado')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_value_progression(history):\n",
    "    \"\"\"\n",
    "    Visualiza a progress√£o do delta ao longo das itera√ß√µes.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(11, 5))\n",
    "    \n",
    "    iterations = [h['iteration'] for h in history]\n",
    "    deltas = [h['delta'] for h in history]\n",
    "    \n",
    "    ax.semilogy(iterations, deltas, 'o-', linewidth=2.5, markersize=8, color='#E74C3C')\n",
    "    ax.axhline(y=1e-2, color='green', linestyle='--', linewidth=2, label='Toler√¢ncia (1e-2)')\n",
    "    \n",
    "    ax.set_xlabel('N√∫mero de Itera√ß√µes', fontweight='bold', fontsize=11)\n",
    "    ax.set_ylabel('Delta (log scale)', fontweight='bold', fontsize=11)\n",
    "    ax.set_title('Converg√™ncia do Value Iteration', fontweight='bold', fontsize=12)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_policy(V):\n",
    "    \"\"\"\n",
    "    Visualiza a pol√≠tica √≥tima (melhor a√ß√£o em cada estado).\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Fundo com valores\n",
    "    im = ax.imshow(np.where(maze == 1, np.nan, V), cmap='RdYlGn', vmin=0, vmax=1, alpha=0.6)\n",
    "    \n",
    "    # Desenhar paredes\n",
    "    for row in range(num_rows):\n",
    "        for col in range(num_cols):\n",
    "            if maze[row, col] == 1:\n",
    "                rect = patches.Rectangle((col-0.5, row-0.5), 1, 1, \n",
    "                                        linewidth=0, facecolor='black', alpha=0.7)\n",
    "                ax.add_patch(rect)\n",
    "    \n",
    "    # Desenhar dire√ß√µes da pol√≠tica √≥tima\n",
    "    arrows = {\n",
    "        'up': (0, -0.3, '‚Üë'),\n",
    "        'down': (0, 0.3, '‚Üì'),\n",
    "        'left': (-0.3, 0, '‚Üê'),\n",
    "        'right': (0.3, 0, '‚Üí')\n",
    "    }\n",
    "    \n",
    "    for row in range(num_rows):\n",
    "        for col in range(num_cols):\n",
    "            if maze[row, col] != 1 and maze[row, col] != 9:\n",
    "                # Encontrar melhor a√ß√£o\n",
    "                best_action = None\n",
    "                best_value = -np.inf\n",
    "                \n",
    "                for action in ['up', 'down', 'left', 'right']:\n",
    "                    action_value = calculate_action_value((row, col), action)\n",
    "                    if action_value > best_value:\n",
    "                        best_value = action_value\n",
    "                        best_action = action\n",
    "                \n",
    "                # Desenhar seta\n",
    "                if best_action:\n",
    "                    dx, dy, arrow = arrows[best_action]\n",
    "                    ax.text(col + dx, row + dy, arrow, ha='center', va='center',\n",
    "                           fontsize=16, fontweight='bold', color='blue')\n",
    "            \n",
    "            elif maze[row, col] == 9:\n",
    "                ax.text(col, row, '‚òÖ', ha='center', va='center',\n",
    "                       fontsize=14, fontweight='bold', color='red')\n",
    "    \n",
    "    ax.set_xticks(range(num_cols))\n",
    "    ax.set_yticks(range(num_rows))\n",
    "    ax.set_xlim(-0.5, num_cols-0.5)\n",
    "    ax.set_ylim(num_rows-0.5, -0.5)\n",
    "    ax.set_title('Pol√≠tica √ìtima (Melhor A√ß√£o em Cada Estado)', fontweight='bold', fontsize=12)\n",
    "    \n",
    "    plt.colorbar(im, ax=ax, label='Valor do Estado')\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "print(\"‚úì Fun√ß√µes de visualiza√ß√£o definidas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3a61ec",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Executar Value Iteration\n",
    "\n",
    "O algoritmo calcular√° o valor de cada estado iterativamente at√© convergir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757c59df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXECUTANDO VALUE ITERATION\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Executar o algoritmo\n",
    "history = value_iteration(max_iterations=50, tolerance=1e-2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTADOS FINAIS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nMatriz Final de Valores (V):\")\n",
    "print(V.round(3))\n",
    "print(f\"\\nMaior valor encontrado: {V.max():.4f}\")\n",
    "print(f\"Menor valor encontrado: {V[V > 0].min():.4f}\")\n",
    "print(f\"N√∫mero de itera√ß√µes: {len(history)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b745f089",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Visualiza√ß√µes dos Resultados\n",
    "\n",
    "### Visualiza√ß√£o 1: Fun√ß√£o de Valor Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5b3653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar a fun√ß√£o de valor final\n",
    "fig1 = plot_value_function(V, title=\"Fun√ß√£o de Valor Final - Itera√ß√£o da Solu√ß√£o\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Interpreta√ß√£o do Mapa de Valores:\")\n",
    "print(\"-\" * 70)\n",
    "print(\"‚Ä¢ Cores VERDES: Estados com alto valor (perto do objetivo)\")\n",
    "print(\"‚Ä¢ Cores VERMELHAS: Estados com baixo valor (longe do objetivo)\")\n",
    "print(\"‚Ä¢ PRETO: Paredes (n√£o acess√≠veis)\")\n",
    "print(\"‚Ä¢ ‚òÖ OBJ: Objetivo (valor = 1.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bafba8",
   "metadata": {},
   "source": [
    "### Visualiza√ß√£o 2: Converg√™ncia do Algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e664ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = plot_value_progression(history)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìà Gr√°fico de Converg√™ncia:\")\n",
    "print(\"-\" * 70)\n",
    "print(\"‚Ä¢ Eixo Y (log): Delta - maior mudan√ßa em qualquer estado\")\n",
    "print(\"‚Ä¢ Eixo X: N√∫mero de itera√ß√£o\")\n",
    "print(\"‚Ä¢ Linha verde: Toler√¢ncia de converg√™ncia (1e-2)\")\n",
    "print(f\"‚Ä¢ O algoritmo parou quando Delta < {1e-2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480a10fb",
   "metadata": {},
   "source": [
    "### Visualiza√ß√£o 3: Pol√≠tica √ìtima (Melhor A√ß√£o em Cada Estado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286bec64",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3 = plot_policy(V)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ Pol√≠tica √ìtima:\")\n",
    "print(\"-\" * 70)\n",
    "print(\"‚Ä¢ Cada seta (‚Üë‚Üì‚Üê‚Üí) representa a melhor a√ß√£o em aquele estado\")\n",
    "print(\"‚Ä¢ Seguir as setas levar√° ao caminho √≥timo at√© ao objetivo\")\n",
    "print(\"‚Ä¢ ‚òÖ = Objetivo (meta final)\")\n",
    "print(\"\\nCom a pol√≠tica √≥tima, o agente sabe exatamente para onde ir!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51868d6b",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ An√°lise e Insights\n",
    "\n",
    "### O Que Aprendemos com Value Iteration?\n",
    "\n",
    "#### 1Ô∏è‚É£ **Fun√ß√£o de Valor Representa \"Proximidade da Meta\"**\n",
    "\n",
    "O algoritmo aprendeu que:\n",
    "- ‚úÖ Estados pr√≥ximos ao objetivo t√™m valores ALTOS (verde)\n",
    "- ‚ùå Estados longe do objetivo t√™m valores BAIXOS (vermelho)\n",
    "\n",
    "Isso corresponde √† ideia intuitiva: \"Quanto mais perto estou do objetivo, maior √© meu valor esperado\"\n",
    "\n",
    "#### 2Ô∏è‚É£ **Converg√™ncia √© Garantida**\n",
    "\n",
    "A Equa√ß√£o de Bellman converge quando:\n",
    "- A recompensa √© limitada\n",
    "- 0 ‚â§ Œ≥ < 1 (desconto futuro)\n",
    "\n",
    "No nosso caso:\n",
    "- Convergiria em **{len(history)} itera√ß√µes** (muito r√°pido!)\n",
    "- Delta diminuiu exponencialmente (veja o gr√°fico log)\n",
    "\n",
    "#### 3Ô∏è‚É£ **A Pol√≠tica √ìtima Emerge Naturalmente**\n",
    "\n",
    "A melhor estrat√©gia (pol√≠tica) √© simplesmente:\n",
    "> **\"Em cada estado, sempre execute a a√ß√£o que maximiza o valor esperado\"**\n",
    "\n",
    "Sem explora√ß√£o aleat√≥ria, sem aprendizado - apenas c√°lculos matem√°ticos!\n",
    "\n",
    "#### 4Ô∏è‚É£ **Transi√ß√µes Estoc√°sticas Importam**\n",
    "\n",
    "- 80% de sucesso + 20% de falha = A√ß√µes incertas\n",
    "- O algoritmo aprende a ser robusto a falhas\n",
    "- N√£o assume execu√ß√£o perfeita\n",
    "\n",
    "### Equa√ß√£o de Bellman em Detalhe\n",
    "\n",
    "$$V^{k+1}(s) = \\max_a \\sum_{s'} P(s'|s,a) [R(s,a,s') + \\gamma V^k(s')]$$\n",
    "\n",
    "Em nosso labirinto:\n",
    "- **P(s'|s,a)** = 0.8 (sucesso) ou 0.2 (falha)\n",
    "- **R(s,a,s')** = 0 (custo apenas de movimento)\n",
    "- **Recompensa** = 1 se s' = objetivo, sen√£o 0\n",
    "- **Œ≥** = 0.9 (valor do futuro)\n",
    "\n",
    "### Aplica√ß√µes Pr√°ticas de Value Iteration\n",
    "\n",
    "| Dom√≠nio | Problema | Objetivo |\n",
    "|---------|----------|----------|\n",
    "| **Rob√≥tica** | Navega√ß√£o | Mover robot do ponto A ao B |\n",
    "| **Games** | Estrat√©gia | Encontrar movimento √≥timo |\n",
    "| **Controle** | Estabiliza√ß√£o | Manter sistema em estado desejado |\n",
    "| **Finan√ßas** | Portf√≥lio | Maximizar riqueza ao longo do tempo |\n",
    "\n",
    "### Compara√ß√£o: Value Iteration vs Policy Iteration\n",
    "\n",
    "| Aspecto | Value Iteration | Policy Iteration |\n",
    "|--------|-----------------|-----------------|\n",
    "| **Velocidade** | Mais itera√ß√µes | Menos itera√ß√µes |\n",
    "| **Custo/Itera√ß√£o** | Menor | Maior (resolve sistema linear) |\n",
    "| **Implementa√ß√£o** | Simples | Complexa |\n",
    "| **Uso** | Ambientes pequenos | Problemas grandes |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7aa833",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Desafios Pr√°ticos para Aprendizado\n",
    "\n",
    "### üöÄ Quest√µes para Reflex√£o\n",
    "\n",
    "1. **E se o labirinto fosse muito maior?** (1000x1000 estados)\n",
    "   - Value Iteration ainda convergiria?\n",
    "   - Qual seria o problema de mem√≥ria?\n",
    "   - ‚Üí Solu√ß√£o: Usar aproxima√ß√£o (fun√ß√£o universal aproximadora / rede neural)\n",
    "\n",
    "2. **E se n√£o conhec√™ssemos o modelo (MDP)?** (Sem transi√ß√µes e recompensas)\n",
    "   - Como aprender a pol√≠tica?\n",
    "   - ‚Üí Solu√ß√£o: Q-Learning, SARSA (aprendizado por experi√™ncia)\n",
    "\n",
    "3. **E se o ambiente mudasse?** (Paredes movem, novos objetivos aparecem)\n",
    "   - Como adaptar a pol√≠tica?\n",
    "   - ‚Üí Solu√ß√£o: Aprendizado cont√≠nuo, replanejamento\n",
    "\n",
    "4. **E se houvesse m√∫ltiplos objetivos?** (Encontrar A, depois B, depois C)\n",
    "   - Como estruturar o problema?\n",
    "   - ‚Üí Solu√ß√£o: Subtarefas, hierarquias, op√ß√µes\n",
    "\n",
    "5. **E se houvesse incerteza maior?** (A√ß√µes falham 50% das vezes)\n",
    "   - Como o algoritmo se comportaria?\n",
    "   - ‚Üí Experimente mudar `transition_success` para 0.5!\n",
    "\n",
    "### üìö Pr√≥ximos T√≥picos Relacionados\n",
    "\n",
    "- **Q-Learning**: Aprende sem conhecer o modelo\n",
    "- **Policy Gradient**: Otimiza a pol√≠tica diretamente\n",
    "- **Deep Reinforcement Learning**: Combina DNNs com RL\n",
    "- **Monte Carlo Tree Search**: Busca em espa√ßos enormes (com sampling)\n",
    "- **Multi-Agent RL**: V√°rios agentes interagindo\n",
    "\n",
    "### üìñ Refer√™ncias\n",
    "\n",
    "- Sutton & Barto: \"Reinforcement Learning: An Introduction\" (Cap√≠tulos 3-4)\n",
    "- David Silver: Lectures on Reinforcement Learning (YouTube)\n",
    "- Berkeley CS 285: Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4be38b",
   "metadata": {},
   "source": [
    "## ‚úÖ Conclus√£o\n",
    "\n",
    "### O que Value Iteration nos ensinou\n",
    "\n",
    "**Value Iteration √© um algoritmo fundamental para resolver Processos de Decis√£o Markovianos** quando:\n",
    "- ‚úÖ Conhecemos o modelo (transi√ß√µes e recompensas)\n",
    "- ‚úÖ O espa√ßo de estados √© razoavelmente pequeno\n",
    "- ‚úÖ Precisamos de converg√™ncia garantida\n",
    "\n",
    "**Equa√ß√£o de Bellman √© a base te√≥rica:**\n",
    "> *Toda decis√£o √≥tima pode ser decomposta em uma recompensa imediata + valor descontado do estado futuro*\n",
    "\n",
    "Isso permite algoritmos como:\n",
    "- üéÆ Game AI (Chess, Go, Video Games)\n",
    "- ü§ñ Robot Control\n",
    "- üí∞ Finan√ßas & Trading\n",
    "- üöó Autonomous Vehicles\n",
    "- üì¶ Supply Chain Optimization\n",
    "\n",
    "### Pr√≥ximo Passo Recomendado\n",
    "\n",
    "**Implemente voc√™ mesmo:**\n",
    "1. Modifique o labirinto (adicione mais paredes, mude objetivo)\n",
    "2. Teste com diferentes valores de Œ≥\n",
    "3. Acompanhe como as setas (pol√≠tica) mudam\n",
    "4. Implemente Policy Iteration para comparar\n",
    "\n",
    "---\n",
    "\n",
    "**\"A matem√°tica dos problemas de decis√£o sequencial √© elegante, poderosa e universal. Uma vez que entendes Bellman, opens portas para IA moderna.\"**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
